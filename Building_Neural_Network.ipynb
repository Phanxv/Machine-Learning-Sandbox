{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Network\n",
    "### Notebook by Phankawee Chulakasian\n",
    " This Notebook follows 'Neural Networks from Scratch' tutorial by sentdex on Youtube"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a single neuron from scratch\n",
    "Neuron taking arbitary numbers as inputs each input has its own weight\n",
    "Then output weighted sum plus bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.2\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.2, 5.1, 2.1] #arbitary inputs\n",
    "weights = [3.1, 1.3, 6.5] #arbitary weight\n",
    "bias = 4.2 #arbitary bias\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.2\n"
     ]
    }
   ],
   "source": [
    "inputs = [1.2, 5.1, 2.1] #arbitary inputs\n",
    "weights = [3.1, 1.3, 6.5] #arbitary weight\n",
    "bias = 4.2\n",
    "output = 0\n",
    "\n",
    "for i in range(0,len(inputs)) :\n",
    "    output += inputs[i]*weights[i]\n",
    "output += bias\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUilding Layer \n",
    "Building Layer with 3 neurons from 4 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 4.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 2.5\n",
    "\n",
    "output =    [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "             inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "             inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building layer using dot product\n",
    "dot(A,B) = A[0]*B[0] + A[1]*B[1] + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "            [0.5, -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2,3,0.5]\n",
    "output = np.dot(weights,inputs) + biases\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching input to improve learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "inputs =   [[1, 2, 3, 2.5],\n",
    "            [2.0, 5.0, -1.0, 2.0],\n",
    "            [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights =  [[0.2, 0.8, -0.5, 1.0],\n",
    "            [0.5, -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2,3,0.5]\n",
    "\n",
    "output = np.dot(inputs, np.array(weights).T) + biases\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating layer object\n",
    "Using Object Oriented python to create a layer of neuron with a customizable number of inputs per neuron and number pof neuron per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X = [[1.0, 2.0, 3.0, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class layer_dense :\n",
    "    def __init__(self, n_inputs, n_neurons) :\n",
    "        self.weights = 0.1*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self,inputs) :\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer1 = layer_dense(4,5)\n",
    "layer2 = layer_dense(5,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function\n",
    "each neuron will have one activation funtion so the output of a function will be 0 or 1 we use non linear function so that we can fit non linear data into the neural network\n",
    "\n",
    "Activation function can be \n",
    "  step function \n",
    "  sigmoid function\n",
    "  rectified linear (ReLU) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_ReLU :\n",
    "    def forward(self, inputs) :\n",
    "        self.output = np.maximum(0,inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the activation function with a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "nnfs.init()\n",
    "\n",
    "X, y = spiral_data(100,3)\n",
    "\n",
    "layer1 = layer_dense(2,5)\n",
    "activation1 = activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "#print(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation\n",
    "Use softmax activation to measure how right or worng the model prediction is by using exponential function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n",
      "[array([1.15985616, 0.91945331]), array([1.15142827, 0.98668476]), array([1.22293021, 0.92968807])]\n"
     ]
    }
   ],
   "source": [
    "E = math.e\n",
    "\n",
    "exp_values = []\n",
    "\n",
    "for output in layer2.output :\n",
    "    exp_values.append(E**output)\n",
    "print(layer2.output)\n",
    "print(exp_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.32817932, 0.32422767]), array([0.32579466, 0.34793556]), array([0.34602601, 0.32783677])]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "norm_base = sum(exp_values)\n",
    "norm_val = []\n",
    "\n",
    "for val in exp_values:\n",
    "    norm_val.append(val / norm_base)\n",
    "\n",
    "print(norm_val)\n",
    "print(sum(norm_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55780834 0.44219166]\n",
      " [0.53852545 0.46147455]\n",
      " [0.5681129  0.4318871 ]]\n"
     ]
    }
   ],
   "source": [
    "exp_values = np.exp(layer2.output)\n",
    "norm_val = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "print(norm_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overflow prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_softmax :\n",
    "    def forward(self, inputs) :\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        prob = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33335462 0.33332726 0.33331808]\n",
      " [0.33337826 0.33333772 0.33328405]\n",
      " [0.33338264 0.33328643 0.33333096]\n",
      " [0.33341601 0.33329672 0.3332873 ]\n",
      " [0.33344433 0.33343863 0.33311707]\n",
      " [0.3334231  0.33354467 0.33303225]\n",
      " [0.33347476 0.3335443  0.332981  ]\n",
      " [0.33350173 0.33355016 0.33294815]\n",
      " [0.33353198 0.33332238 0.33314565]\n",
      " [0.33355126 0.33356908 0.33287966]\n",
      " [0.33342123 0.33367047 0.3329083 ]\n",
      " [0.33327818 0.33354437 0.3331774 ]\n",
      " [0.33327246 0.3335583  0.3331692 ]\n",
      " [0.3333889  0.33371934 0.33289176]\n",
      " [0.3331794  0.33329058 0.33352998]\n",
      " [0.33307502 0.33269185 0.33423316]\n",
      " [0.3334395  0.3338324  0.3327281 ]\n",
      " [0.3334351  0.3338536  0.33271128]\n",
      " [0.33302626 0.33271387 0.33425987]\n",
      " [0.33312243 0.3332531  0.3336245 ]\n",
      " [0.33310062 0.3332022  0.3336972 ]\n",
      " [0.3330262  0.33290333 0.33407047]\n",
      " [0.33313808 0.33342645 0.33343545]\n",
      " [0.33316275 0.33355123 0.33328605]\n",
      " [0.33307013 0.3332348  0.33369508]\n",
      " [0.3329541  0.3323925  0.33465338]\n",
      " [0.33309886 0.33342278 0.3334784 ]\n",
      " [0.33288452 0.33222055 0.33489493]\n",
      " [0.33286965 0.33242586 0.33470446]\n",
      " [0.33282137 0.3321269  0.33505175]\n",
      " [0.3329707  0.3324336  0.3345957 ]\n",
      " [0.33302623 0.332571   0.33440277]\n",
      " [0.33294812 0.33237773 0.33467418]\n",
      " [0.3328318  0.33252692 0.33464128]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33305064 0.33263147 0.3343179 ]\n",
      " [0.33307964 0.33270338 0.334217  ]\n",
      " [0.3333415  0.33332556 0.33333296]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33325568 0.33314013 0.33360425]\n",
      " [0.33323154 0.33308023 0.3336882 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33350652 0.33316848 0.33332494]\n",
      " [0.33353814 0.3331384  0.33332348]\n",
      " [0.3330233  0.3325638  0.33441287]\n",
      " [0.33347037 0.33320287 0.33332673]\n",
      " [0.33374062 0.3329458  0.33331358]\n",
      " [0.33329973 0.33324975 0.33345056]\n",
      " [0.33354536 0.33313155 0.33332312]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3338077  0.332882   0.33331028]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33362806 0.33305284 0.33331907]\n",
      " [0.33377042 0.33291745 0.33331215]\n",
      " [0.3338296  0.33286119 0.33330923]\n",
      " [0.3346134  0.33344918 0.33193737]\n",
      " [0.33446473 0.3325271  0.3330081 ]\n",
      " [0.33425426 0.3324575  0.33328828]\n",
      " [0.33414185 0.3325643  0.33329383]\n",
      " [0.3344575  0.33229426 0.33324826]\n",
      " [0.33429256 0.33242112 0.33328635]\n",
      " [0.334751   0.33447546 0.3307736 ]\n",
      " [0.3342657  0.33557603 0.3301583 ]\n",
      " [0.3346892  0.33512506 0.3301857 ]\n",
      " [0.33482295 0.33448106 0.330696  ]\n",
      " [0.33486536 0.3340732  0.33106145]\n",
      " [0.33488834 0.3340281  0.33108357]\n",
      " [0.3334134  0.33507818 0.33150843]\n",
      " [0.33492404 0.3344031  0.33067286]\n",
      " [0.33313134 0.33487442 0.3319942 ]\n",
      " [0.33497873 0.33419785 0.33082342]\n",
      " [0.3347504  0.3356774  0.32957217]\n",
      " [0.3350231  0.33393013 0.33104673]\n",
      " [0.33392596 0.33563218 0.3304418 ]\n",
      " [0.33494982 0.33529574 0.32975438]\n",
      " [0.33233437 0.33220804 0.33545762]\n",
      " [0.33291072 0.33449736 0.33259192]\n",
      " [0.33269992 0.3337903  0.33350977]\n",
      " [0.33272448 0.33391517 0.33336034]\n",
      " [0.33402976 0.3358573  0.3301129 ]\n",
      " [0.3325575  0.33335885 0.33408365]\n",
      " [0.33196115 0.3299556  0.3380832 ]\n",
      " [0.33237025 0.3326725  0.3349572 ]\n",
      " [0.33288327 0.33463722 0.3324795 ]\n",
      " [0.33368692 0.33572027 0.33059284]\n",
      " [0.33240372 0.33293363 0.33466268]\n",
      " [0.3329685  0.33501527 0.3320162 ]\n",
      " [0.33192518 0.32986808 0.3382068 ]\n",
      " [0.33196735 0.32997078 0.33806184]\n",
      " [0.33201054 0.33130068 0.3366888 ]\n",
      " [0.33204502 0.33016005 0.33779493]\n",
      " [0.33174548 0.32971022 0.33854425]\n",
      " [0.3318981  0.3298021  0.33829987]\n",
      " [0.33221027 0.3305635  0.33722627]\n",
      " [0.33186963 0.3297329  0.3383975 ]\n",
      " [0.33249965 0.33127248 0.3362279 ]\n",
      " [0.33168256 0.3297207  0.33859676]\n",
      " [0.3325622  0.33142617 0.33601162]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33331656 0.33329555 0.33338788]\n",
      " [0.3333006  0.3332518  0.33344766]\n",
      " [0.3332857  0.3332148  0.33349958]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3332775  0.3331945  0.333528  ]\n",
      " [0.3333741  0.33329454 0.33333138]\n",
      " [0.33338305 0.333286   0.33333093]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33345178 0.33322057 0.33332762]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33331212 0.33328056 0.33340737]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33353642 0.33314008 0.33332354]\n",
      " [0.33346918 0.33320403 0.33332676]\n",
      " [0.33361948 0.33306098 0.3333195 ]\n",
      " [0.3335485  0.33312857 0.33332294]\n",
      " [0.333632   0.33304912 0.3333189 ]\n",
      " [0.33378732 0.3336534  0.33255932]\n",
      " [0.33380902 0.33342686 0.33276412]\n",
      " [0.33366686 0.33301595 0.3333172 ]\n",
      " [0.3338073  0.3331155  0.33307716]\n",
      " [0.33385974 0.3332792  0.3328611 ]\n",
      " [0.33390436 0.33354142 0.33255422]\n",
      " [0.33390194 0.33326325 0.3328348 ]\n",
      " [0.33388144 0.33304158 0.33307704]\n",
      " [0.3339682  0.3334665  0.33256528]\n",
      " [0.33397266 0.33329105 0.33273625]\n",
      " [0.3336426  0.33430248 0.33205488]\n",
      " [0.33378097 0.33441642 0.33180264]\n",
      " [0.33405426 0.3339104  0.3320353 ]\n",
      " [0.33396006 0.3344337  0.3316062 ]\n",
      " [0.33405107 0.3342418  0.33170712]\n",
      " [0.33336648 0.334211   0.3324225 ]\n",
      " [0.3333295  0.33420306 0.3324674 ]\n",
      " [0.3335168  0.33438346 0.3320997 ]\n",
      " [0.33318716 0.33409354 0.33271933]\n",
      " [0.33318612 0.33412257 0.3326913 ]\n",
      " [0.33295515 0.33334738 0.33369744]\n",
      " [0.3330854  0.33385706 0.33305752]\n",
      " [0.33314052 0.33407363 0.33278584]\n",
      " [0.3334947  0.33451256 0.33199272]\n",
      " [0.33410078 0.33494142 0.3309578 ]\n",
      " [0.3329726  0.33359402 0.33343342]\n",
      " [0.33315477 0.334249   0.3325963 ]\n",
      " [0.33279145 0.33295023 0.33425832]\n",
      " [0.33251253 0.33140537 0.33608213]\n",
      " [0.33262315 0.33224285 0.33513397]\n",
      " [0.33271533 0.33274248 0.33454219]\n",
      " [0.33278102 0.33306882 0.33415014]\n",
      " [0.33291176 0.33361712 0.33347115]\n",
      " [0.33267868 0.33171278 0.33560854]\n",
      " [0.3328719  0.3335401  0.333588  ]\n",
      " [0.33250898 0.33196074 0.33553028]\n",
      " [0.3329118  0.33375758 0.33333066]\n",
      " [0.33239916 0.33102602 0.33657476]\n",
      " [0.33241883 0.33107418 0.33650696]\n",
      " [0.33243743 0.3311198  0.3364428 ]\n",
      " [0.33259857 0.3315156  0.3358858 ]\n",
      " [0.33240333 0.33103618 0.3365605 ]\n",
      " [0.33241925 0.33107525 0.33650547]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33279175 0.33199146 0.33521676]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33272526 0.33182758 0.33544713]\n",
      " [0.3335646  0.33311328 0.33332217]\n",
      " [0.33363506 0.33304623 0.33331874]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33299392 0.332491   0.33451504]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33347207 0.3332013  0.33332667]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33385742 0.33283466 0.33330786]\n",
      " [0.33388487 0.3328086  0.33330652]\n",
      " [0.3340399  0.33266124 0.3332989 ]\n",
      " [0.3347023  0.3320319  0.33326575]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33490846 0.3321881  0.33290344]\n",
      " [0.33412012 0.33258495 0.3332949 ]\n",
      " [0.33448172 0.33224142 0.33327687]\n",
      " [0.3343567  0.33236015 0.33328313]\n",
      " [0.33492532 0.33192736 0.33314735]\n",
      " [0.3338887  0.33280498 0.33330637]\n",
      " [0.33484924 0.33189252 0.3332583 ]\n",
      " [0.33498195 0.33187902 0.33313906]\n",
      " [0.3352171  0.3356657  0.32911724]\n",
      " [0.33538622 0.3339866  0.33062717]\n",
      " [0.33531767 0.33543327 0.32924905]\n",
      " [0.33543316 0.33452708 0.3300398 ]\n",
      " [0.33531633 0.33276427 0.33191943]\n",
      " [0.3349467  0.33676428 0.32828903]\n",
      " [0.33550128 0.3340522  0.3304465 ]\n",
      " [0.33548477 0.33514008 0.32937518]\n",
      " [0.33508965 0.33673406 0.32817626]\n",
      " [0.33357668 0.3359237  0.33049962]\n",
      " [0.33511123 0.3368333  0.32805547]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3333526  0.33336622 0.3332812 ]\n",
      " [0.33332652 0.33337563 0.3332979 ]\n",
      " [0.3333483  0.33341846 0.33323324]\n",
      " [0.33328152 0.33327478 0.3334437 ]\n",
      " [0.33330953 0.33341852 0.33327192]\n",
      " [0.3334584  0.33350083 0.33304077]\n",
      " [0.33323687 0.3332028  0.33356032]\n",
      " [0.3332751  0.33340105 0.33332384]\n",
      " [0.33328745 0.33347675 0.3332358 ]\n",
      " [0.33316657 0.33296528 0.3338682 ]\n",
      " [0.33322382 0.33331543 0.33346075]\n",
      " [0.33314076 0.33295104 0.3339082 ]\n",
      " [0.33320695 0.3333244  0.33346862]\n",
      " [0.33318406 0.33327037 0.3335456 ]\n",
      " [0.3331718  0.33293194 0.33389628]\n",
      " [0.33311778 0.3330596  0.33382264]\n",
      " [0.33307138 0.3328635  0.3340651 ]\n",
      " [0.3331494  0.33328506 0.33356553]\n",
      " [0.33321935 0.33304992 0.33373073]\n",
      " [0.33306715 0.33267236 0.33426046]\n",
      " [0.33305883 0.33265182 0.3342893 ]\n",
      " [0.33319256 0.33298343 0.33382404]\n",
      " [0.33294523 0.3324505  0.3346043 ]\n",
      " [0.33299664 0.33249775 0.33450565]\n",
      " [0.3330847  0.3327159  0.33419943]\n",
      " [0.33290413 0.332269   0.3348269 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33320484 0.33301386 0.33378127]\n",
      " [0.3331753  0.33294055 0.33388418]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33315787 0.33289734 0.33394474]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33400336 0.33297706 0.3330196 ]\n",
      " [0.33337155 0.33329695 0.3333315 ]\n",
      " [0.33376718 0.33292052 0.33331227]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33356193 0.3331158  0.3333223 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33319584 0.33299157 0.3338126 ]\n",
      " [0.33392125 0.33277404 0.33330473]\n",
      " [0.33396548 0.33273193 0.33330253]\n",
      " [0.33407202 0.33263066 0.3332973 ]\n",
      " [0.33393535 0.33276063 0.33330402]\n",
      " [0.33436242 0.3338314  0.33180618]\n",
      " [0.33404773 0.33265376 0.3332985 ]\n",
      " [0.3339841  0.33271423 0.33330163]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3342584  0.3325476  0.33319396]\n",
      " [0.334475   0.3337356  0.33178943]\n",
      " [0.33442107 0.3330271  0.33255184]\n",
      " [0.33447173 0.33451802 0.33101022]\n",
      " [0.33376095 0.33496308 0.33127594]\n",
      " [0.33455348 0.33352855 0.33191797]\n",
      " [0.33433396 0.33526084 0.33040524]\n",
      " [0.33434355 0.33237353 0.33328292]\n",
      " [0.33423868 0.33537313 0.3303882 ]\n",
      " [0.33457002 0.33482274 0.33060724]\n",
      " [0.33461046 0.3347557  0.33063385]\n",
      " [0.33456346 0.33506653 0.33036998]\n",
      " [0.3345124  0.33530858 0.33017904]\n",
      " [0.3345125  0.33538914 0.3300984 ]\n",
      " [0.33392558 0.33533114 0.33074328]\n",
      " [0.33412507 0.33549    0.33038494]\n",
      " [0.3346597  0.3352308  0.33010948]\n",
      " [0.33304048 0.3345311  0.33242846]\n",
      " [0.33396107 0.33545673 0.33058223]\n",
      " [0.3325067  0.3325927  0.3349006 ]\n",
      " [0.33293897 0.33429348 0.33276764]\n",
      " [0.3327335  0.33359474 0.33367172]\n",
      " [0.33464652 0.33577102 0.32958242]\n",
      " [0.33354115 0.3352589  0.3312    ]\n",
      " [0.3321326  0.33084372 0.3370237 ]\n",
      " [0.3327289  0.33372036 0.33355072]\n",
      " [0.33250257 0.33286673 0.33463064]\n",
      " [0.3322808  0.331901   0.3358182 ]\n",
      " [0.33226538 0.33069825 0.33703637]\n",
      " [0.33200806 0.3300699  0.33792204]\n",
      " [0.3324044  0.33103877 0.33655685]\n",
      " [0.33197248 0.33024454 0.337783  ]\n",
      " [0.3320752  0.33023366 0.33769116]\n",
      " [0.33208287 0.3311754  0.33674172]\n",
      " [0.3327733  0.331946   0.33528075]\n",
      " [0.33224395 0.33064577 0.33711028]\n",
      " [0.3323034  0.33079138 0.3369052 ]\n",
      " [0.33259392 0.3315042  0.33590183]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33247805 0.3312195  0.33630246]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33231026 0.33080807 0.3368817 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33392274 0.3327726  0.33330464]\n",
      " [0.33297956 0.33245555 0.3345649 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33346745 0.33320567 0.33332688]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33279818 0.33200732 0.3351945 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3339547  0.33274218 0.33330306]]\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = layer_dense(2,4)\n",
    "activation1 = activation_ReLU()\n",
    "\n",
    "dense2 = layer_dense(4,3)\n",
    "activation2 = Activation_softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "Use to find confidence score of the model \n",
    "Example mean absolute error\n",
    "Categorical Crosss-Entropy \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding \n",
    "Vector with N class long filled with 0 except at the target label have a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n",
      "0.35667494393873245\n",
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "        (math.log(softmax_output[1]))*target_output[1] +\n",
    "        (math.log(softmax_output[2]))*target_output[2])\n",
    "\n",
    "print(loss)\n",
    "\n",
    "loss = -(math.log(softmax_output[0]))\n",
    "print(loss)\n",
    "\n",
    "softmax_output = [0.5, 0.1, 0.4]\n",
    "\n",
    "loss = -(math.log(softmax_output[0]))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35667494 0.69314718 0.10536052]]\n"
     ]
    }
   ],
   "source": [
    "softmax_output = np.array([[0.7,0.1,0.2],\n",
    "                          [0.1,0.5,0.4],\n",
    "                          [0.02,0.9,0.08]])\n",
    "\n",
    "class_target = [0,1,1]\n",
    "\n",
    "print(-np.log(softmax_output[[0,1,2],[class_target]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss :\n",
    "    def calculate(self, output, y) :\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "class Loss_CategoricalCrossEntropy(Loss) :\n",
    "    def forward(self, y_pred, y_true) :\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        if len(y_true.shape) == 1 :\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2 :\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        negative_log_likelihood = -np.log(correct_confidences)\n",
    "        return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  1.0995139\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss =loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(\"Loss : \",loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89e21cb9b70143694913886210e203ea6c606aeee4b5a5a8f6b1477840a25e1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
